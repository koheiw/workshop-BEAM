---
title: "Longitudinal analysis of economic news using Quanteda"
author: "Kohei Watanabe (Waseda, LSE)"
date: "6 June 2018"
output: 
    ioslides_presentation:
        css: "images/ioslides_styles.css"
        logo: "images/quanteda-logo.png"
        widescreen: true
        toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    fig.width = 10.5,
    fig.height = 5.5,
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
)
require(quanteda)
```

# What is Quanteda?

## Quanteda is an R package

**quanteda** is an R package for quantitative text analysis developed by a team based at the LSE.

- **quanteda** stands for **qu**antetative **an**alysis of **te**xtual **da**ta
- The package offers a set of functions for quantitative text analysis
    - See https://docs.quanteda.io/reference for the list of functions
- Developed for high *consistency* and *performance*
    - Many of the core functions are written in C++ with multi-threading
    - Faster than other R packages (**tm** or **tidytext**) and even Python package (**gensim**)
    - Works with Chinese, Japanese and Korean texts
- Used by leading political scientists in North America, Europe and Asia
    - Analyze party manifestos, legislative speeches, news articles, social media etc.

## Quanteda team

```{r echo=FALSE, out.height="500px", out.width="auto"}
knitr::include_graphics("images/team.png")
```

## What is quantitative text analysis

In quantitative text analysis, use the same technology as natural language processing (NLP) but for different goals.

- We try to discover *theoretically interesting* patterns from a social scientific point of view
    - Replication of manual reading of text is not the goal
    - Social scientists are interested in specific aspects of textual data
- Analytic methods vary from simple frequency analysis to machine learning
    - Dictionary analysis is probably the most popular approach
    - Machine learning is becoming more popular
        
## Cost-control trade off in machine learning

Not easy to automate theoretically grounded analysis because of the cost-control trade off. 

- Cost for training complex model is particularly high
- Supervised model can be theoretical but usually expensive
    - naive Bayes, Wordscores, random forest, SVM, neural network
- Unsupervised models inexpensive but often atheoretical
    - topic models, correspondence analysis, Wordfish
- Semi-supervised models try to balance between theory and cost
    - Newsmap, LSS
        
# Examples

## Sentiment analysis of news

- Lexicoder Sentiment Dictionary (LSD)
    - Widely used in political communication research
    - Created by Young and Soroka to analyze political news in North America
    ```{r}
    lengths(data_dictionary_LSD2015)
    ```
- Latent Semantic Scaling (LSS)
    - Parameters are estimated on the corpus to increase internal validity
    - Uses "seed words" to identify subject-specific sentiment words

## Lexicoder Sentiment Dictionary

```{r, echo=FALSE}
# Corpus is available at https://www.dropbox.com/s/kfhdoifes7z7t6j/data_corpus_guardian2016-10k.rds?dl=0
```

```{r}
corp <- readRDS('/home/kohei/Dropbox/Public/data_corpus_guardian2016-10k.rds')
ndoc(corp)
range(docvars(corp, "date"))

mt <- dfm(corp, remove_punc = TRUE, remove = stopwords())
mt_dict <- dfm_lookup(mt, data_dictionary_LSD2015[1:2])
sent <- (mt_dict[,2] - mt_dict[,1]) / (rowSums(mt) + 1)

data <- data.frame(date = docvars(mt_dict, "date"),
                   lsd = as.numeric(scale(sent)))
```

---

```{r}
dim(mt)
head(mt[1:6, 1:6])
```

---

```{r}
dim(mt_dict)
head(mt_dict)
```

## Sentiment of news around Brexit vote by LSD

```{r, echo=FALSE}
plot(data$date, data$lsd, pch = 16, col = rgb(0, 0, 0, 0.05),
     ylim = c(-1, 1), ylab = 'sentiment')
lines(lowess(data$date, data$lsd, f = 0.01), col = 1)
abline(h = 0, v = as.Date("2016-06-23"), lty = c(1, 3))
```

## Latent Semantic Scaling

```{r, eval=FALSE}
devtools::install_github("koheiw/LSS")
```

```{r, cache=TRUE, message=FALSE}
require(LSS)
toks_sent <- corp %>% 
    corpus_reshape('sentences') %>% 
    tokens(remove_punct = TRUE)
mt_sent <- toks_sent %>% 
    dfm(remove = stopwords()) %>% 
    dfm_select('^[0-9a-zA-Z]+$', valuetype = 'regex') %>% 
    dfm_trim(min_termfreq = 5)

eco <- head(char_keyness(toks_sent, 'econom*', window = 10), 500)
lss <- textmodel_lss(mt_sent, seedwords('pos-neg'), features = eco, cache = TRUE)
```

## Sentiment seed words

```{r}
seedwords('pos-neg')
```

## Economic sentiment words

```{r}
head(coef(lss), 20) # most positive words
```

---

```{r}
tail(coef(lss), 20) # most negative words
```

## Sentiment of news around Brexit vote by LSS

```{r, echo=FALSE}
data$lss <- predict(lss, newdata = mt)
plot(data$date, data$lss, pch = 16, col = rgb(0, 0, 0, 0.05),
     ylim = c(-1, 1), ylab = 'economic sentiment')
lines(lowess(data$date, data$lss, f = 0.01), col = 1)
abline(h = 0, v = as.Date("2016-06-23"), lty = c(1, 3))
```

## Compare LSD and LSS

```{r, echo=FALSE}
plot(data$date, rep(0, nrow(data)), pch = 16, col = rgb(0, 0, 0, 0.05),
     ylim = c(-1, 1), ylab = 'sentiment', type = "n")
lines(lowess(data$date, data$lss, f = 0.01), col = 1)
lines(lowess(data$date, data$lsd, f = 0.01), col = 2)
abline(h = 0, v = as.Date("2016-06-23"), lty = c(1, 3))
legend("topright", lty = 1, col = 1:2, legend = c("LSS", "LSD"))
```

## Seed words for different dimensions

The sentiment seed words are already available, but you can also make you own seed words.

```{r}
# concern
seed_concern <- c("concern*", "worr*", "anxi*")

# weakening
seed_weakening <- c("declin*", "weak*", "retreat*")

# indicator vs consequence
seed_ecoframe <- c('рост*' = 1, 'инфляци*' = 1, 'безработиц*' = 1,
                 # 'growth'     'inflation'   'unemployment'
                   'рубл*' = 1, 'бедност*' = -1, 'сокращени* доходов' = -1,
                 # 'currency'  'poverty'        'wage reduction'
                   'забастовк*' = -1, 'увольнени* работник*' = 1, 'потер*' = -1)
                 # 'strikes'          'layoff'                    'economic loss'
```

## Conclusions

You can locate docents on sentiment or any other dimensions at low cost using LSS.

- It is an ideal tool to generate time series data from news articles
- It can be used in economic research along with macro-economic data

When you use LSS, please be aware that

- It requires large corpus of texts (usually 5000 or more full-text articles)
- It is affected by how texts are tokenized (punctuations, function words etc.)
- Its individual prediction is not very accurate (compared to fully-supervised models)

## More about Quanteda

- Quanteda Documentation: https://docs.quanteda.io
- Quanteda Tutorials: https://tutorials.quanteda.io
    - Overview
    - Data import
    - Basic operations
    - Statistical analysis
    - Advanced operations
    - Scaling and classification
- These slides: https://github.com/koheiw/workshop-BEAM
- There might be introductory text analysis workshop using Quanteda at Waseda

